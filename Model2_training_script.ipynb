{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skript for training the MC based regression network\n",
    "\n",
    "    1. Loading necessary data\n",
    "    2. Initialize model\n",
    "    3. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "annotation_dict = pickle.load(open(\"wsis/meningioma_grading_training_dataset/annotation_dict2.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_to_discard = list()\n",
    "keys_for_training = list()\n",
    "keys_excluded_testset=0\n",
    "ldfnummer_exclude = []\n",
    "testset_exclude = pickle.load(open('Testset_List.p','rb'))\n",
    "for key in annotation_dict:\n",
    "    if key in testset_exclude:\n",
    "        ldfnummer_exclude += [annotation_dict[key]['lfd_number']]\n",
    "\n",
    "for key in annotation_dict:\n",
    "    try:\n",
    "        if (annotation_dict[key]['mitotic_count'] >= 0) and (annotation_dict[key]['lfd_number'] not in ldfnummer_exclude):\n",
    "            keys_for_training.append(key)\n",
    "        else:\n",
    "            keys_to_discard.append(key)\n",
    "    except KeyError:\n",
    "        keys_to_discard.append(key)\n",
    "\n",
    "keys_excluded_testset,\n",
    "ldfnummer_exclude = np.unique(ldfnummer_exclude)\n",
    "ldfnummer_exclude\n",
    "len(ldfnummer_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the highest mitotic count of a patient is used in training\n",
    "tmp = dict()\n",
    "for key in annotation_dict:\n",
    "    lfd_number = annotation_dict[key]['lfd_number']\n",
    "    if (lfd_number in ldfnummer_exclude):\n",
    "        continue\n",
    "    if lfd_number not in tmp:\n",
    "        tmp[lfd_number] = dict()\n",
    "        tmp[lfd_number]['file'] = key\n",
    "        tmp[lfd_number]['who_grade'] = annotation_dict[key]['who_grade']\n",
    "        try:\n",
    "            tmp[lfd_number]['mitotic_count'] = annotation_dict[key]['mitotic_count']\n",
    "        except KeyError:\n",
    "            tmp[lfd_number]['mitotic_count'] = 0\n",
    "    try:\n",
    "        if tmp[lfd_number]['mitotic_count'] < annotation_dict[key]['mitotic_count']:\n",
    "            tmp[lfd_number]['file'] = key\n",
    "            tmp[lfd_number]['who_grade'] = annotation_dict[key]['who_grade']\n",
    "            tmp[lfd_number]['mitotic_count'] = annotation_dict[key]['mitotic_count']\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "patients = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the mitotic count\n",
    "mcs = [patients[key]['mitotic_count'] for key in patients]\n",
    "mean_mcs = np.mean(mcs)\n",
    "std_mcs = np.std(mcs)\n",
    "mcs = [(m-mean_mcs)/std_mcs for m in mcs]\n",
    "# Normalize\n",
    "max_mcs = max(mcs)\n",
    "mcs = [m/max_mcs for m in mcs]\n",
    "for key,m in zip (patients, mcs):\n",
    "    patients[key]['mitotic_count'] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([309.,  18.,   8.,   2.,   1.,   2.,   0.,   0.,   0.,   1.]),\n",
       " array([-0.05500172,  0.05049845,  0.15599862,  0.2614988 ,  0.36699897,\n",
       "         0.47249914,  0.57799931,  0.68349948,  0.78899966,  0.89449983,\n",
       "         1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(6)\n",
    "# 15 % validation data\n",
    "val_numbers = [lfd_number for lfd_number in random.sample(patients.keys(),54)]\n",
    "train_numbers = [lfd_number for lfd_number in patients if lfd_number not in val_numbers]\n",
    "mcs_train = [patients[lfd_number]['mitotic_count'] for lfd_number in train_numbers]\n",
    "grades_train = [patients[lfd_number]['who_grade'] for lfd_number in train_numbers]\n",
    "\n",
    "mcs_val = [patients[lfd_number]['mitotic_count'] for lfd_number in val_numbers]\n",
    "grades_val = [patients[lfd_number]['who_grade'] for lfd_number in val_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mcs_train = torch.tensor(mcs_train, dtype = torch.float32).reshape(-1,1)\n",
    "grades_train = torch.tensor(grades_train, dtype = torch.float32).reshape(-1,1)\n",
    "\n",
    "mcs_val = torch.tensor(mcs_val, dtype = torch.float32).reshape(-1,1)\n",
    "grades_val = torch.tensor(grades_val, dtype = torch.float32).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model2\n",
    "\n",
    "model = Model2.SimpleRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_data,train_labels, model, loss_fn, optimizer, batch_size = 1):\n",
    "    size = len(train_data)\n",
    "    running = list()\n",
    "    for  batch, (X, y) in enumerate(zip(train_data,train_labels)):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        # loss pro item\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running.append(loss.item())\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # current loss\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print(f\"Mean Training Loss: {sum(running)/len(running)}\")\n",
    "    return sum(running)/len(running)\n",
    "  \n",
    "            \n",
    "def test_loop(test_data,test_labels, model, loss_fn):\n",
    "    size = len(test_data)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in zip(test_data,test_labels):\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRegressionModel()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction = 'mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.1,patience=5,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "# function for sampling the data like for the other regression models\n",
    "def sample_train_data(mcs_train,grades_train,train_data_length):\n",
    "    train_vektor = list()\n",
    "    train_labels = list()\n",
    "    for i in range(train_data_length):\n",
    "        grade = random.randint(1,4,1)\n",
    "        train_vektor.append(mcs_train[np.random.choice(np.where(grades_train == float(grade))[0],1)])\n",
    "        train_labels.append(grades_train[np.random.choice(np.where(grades_train == float(grade))[0],1)])\n",
    "    return train_vektor,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.528661  [    0/  300]\n",
      "loss: 1.851999  [  100/  300]\n",
      "loss: 1.193662  [  200/  300]\n",
      "Mean Training Loss: 0.7679080195900921\n",
      "Avg loss: 0.474115 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.032006  [    0/  300]\n",
      "loss: 0.505287  [  100/  300]\n",
      "loss: 0.504113  [  200/  300]\n",
      "Mean Training Loss: 0.36574619974814293\n",
      "Avg loss: 0.271852 \n",
      "\n",
      "Best model saved in Epoch 2\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.195310  [    0/  300]\n",
      "loss: 0.339838  [  100/  300]\n",
      "loss: 0.488459  [  200/  300]\n",
      "Mean Training Loss: 0.278512299869714\n",
      "Avg loss: 0.280430 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.109515  [    0/  300]\n",
      "loss: 0.159989  [  100/  300]\n",
      "loss: 0.026877  [  200/  300]\n",
      "Mean Training Loss: 0.25181460207715645\n",
      "Avg loss: 0.200913 \n",
      "\n",
      "Best model saved in Epoch 4\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.390163  [    0/  300]\n",
      "loss: 0.072854  [  100/  300]\n",
      "loss: 0.418624  [  200/  300]\n",
      "Mean Training Loss: 0.25838843808926565\n",
      "Avg loss: 0.192665 \n",
      "\n",
      "Best model saved in Epoch 5\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.013124  [    0/  300]\n",
      "loss: 0.245518  [  100/  300]\n",
      "loss: 0.217899  [  200/  300]\n",
      "Mean Training Loss: 0.2399757089319443\n",
      "Avg loss: 0.552483 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.171012  [    0/  300]\n",
      "loss: 0.382547  [  100/  300]\n",
      "loss: 0.004295  [  200/  300]\n",
      "Mean Training Loss: 0.26836781468811144\n",
      "Avg loss: 0.295763 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.001247  [    0/  300]\n",
      "loss: 0.465430  [  100/  300]\n",
      "loss: 0.343186  [  200/  300]\n",
      "Mean Training Loss: 0.26282911189148916\n",
      "Avg loss: 0.144709 \n",
      "\n",
      "Best model saved in Epoch 8\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.063433  [    0/  300]\n",
      "loss: 0.330622  [  100/  300]\n",
      "loss: 0.026673  [  200/  300]\n",
      "Mean Training Loss: 0.24298900054731803\n",
      "Avg loss: 0.124856 \n",
      "\n",
      "Best model saved in Epoch 9\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000203  [    0/  300]\n",
      "loss: 0.208689  [  100/  300]\n",
      "loss: 1.692060  [  200/  300]\n",
      "Mean Training Loss: 0.2845147617694788\n",
      "Avg loss: 0.192996 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.142733  [    0/  300]\n",
      "loss: 0.089228  [  100/  300]\n",
      "loss: 0.303744  [  200/  300]\n",
      "Mean Training Loss: 0.23025961628336516\n",
      "Avg loss: 0.182572 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.870007  [    0/  300]\n",
      "loss: 0.128662  [  100/  300]\n",
      "loss: 0.732390  [  200/  300]\n",
      "Mean Training Loss: 0.23075462023661203\n",
      "Avg loss: 0.248219 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.267323  [    0/  300]\n",
      "loss: 0.022725  [  100/  300]\n",
      "loss: 0.028932  [  200/  300]\n",
      "Mean Training Loss: 0.21059442763875494\n",
      "Avg loss: 0.195380 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.003346  [    0/  300]\n",
      "loss: 0.013986  [  100/  300]\n",
      "loss: 0.003672  [  200/  300]\n",
      "Mean Training Loss: 0.21700109522540345\n",
      "Avg loss: 0.174908 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.005234  [    0/  300]\n",
      "loss: 0.031669  [  100/  300]\n",
      "loss: 0.487695  [  200/  300]\n",
      "Mean Training Loss: 0.23967477309127883\n",
      "Avg loss: 0.227482 \n",
      "\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.267017  [    0/  300]\n",
      "loss: 0.017520  [  100/  300]\n",
      "loss: 0.178117  [  200/  300]\n",
      "Mean Training Loss: 0.1997650424642264\n",
      "Avg loss: 0.174142 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.005286  [    0/  300]\n",
      "loss: 0.001111  [  100/  300]\n",
      "loss: 1.271423  [  200/  300]\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "epochs = 50\n",
    "losses = list()\n",
    "train_loss = list()\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    sampled_mcs,sampled_labels = sample_train_data(mcs_train,grades_train,300)\n",
    "    train_loss.append(train_loop(sampled_mcs,sampled_labels, model, loss_fn, optimizer))\n",
    "    sampled_mcs,sampled_labels = sample_train_data(mcs_val,grades_val,100)\n",
    "    val_loss = test_loop(sampled_mcs,sampled_labels, model, loss_fn)\n",
    "    scheduler.step(val_loss)\n",
    "    losses.append(val_loss)\n",
    "    \n",
    "    # model selection\n",
    "    if t == 0:\n",
    "        best_loss = val_loss\n",
    "    elif best_loss > val_loss:\n",
    "        print(f\"Best model saved in Epoch {t+1}\")\n",
    "        best_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "        \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.tensor(np.arange(min(mcs_train),max(mcs_train),0.01),dtype = torch.float32).reshape(-1,1)\n",
    "with torch.no_grad():\n",
    "    out = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(test_data,out.numpy())\n",
    "plt.xlabel('Mitotic Count')\n",
    "plt.ylabel('Kontinuierlicher WHO-Grad')\n",
    "plt.title('Function learnd by Regression Network')\n",
    "plt.scatter(sampled_mcs,sampled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses,label = 'val loss')\n",
    "plt.plot(train_loss, label = 'train loss')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
